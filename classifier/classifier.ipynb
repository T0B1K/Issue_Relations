{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "baseline.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c7sQLhr4vfzq",
        "outputId": "9445e0b8-e67b-4ec0-eb95-708d7fd184fb"
      },
      "source": [
        "!unzip trainingsdata.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  trainingsdata.zip\n",
            "  inflating: y_train.npy             \n",
            "  inflating: y_val.npy               \n",
            "  inflating: X_test.csv              \n",
            "  inflating: X_train.csv             \n",
            "  inflating: X_val.csv               \n",
            "  inflating: y_test.npy              \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "81yczvs7crZC",
        "outputId": "be4bd3f7-5f9c-4fd5-f698-6d52c34709f4"
      },
      "source": [
        "!unzip data.zip"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Archive:  data.zip\n",
            "  inflating: lda_train.npy           \n",
            "  inflating: lda_val.npy             \n",
            "  inflating: X_test_ft_avg.npy       \n",
            "  inflating: X_test_ft_usif.npy      \n",
            "  inflating: X_test_glove_avg.npy    \n",
            "  inflating: X_test_glove_usif.npy   \n",
            "  inflating: X_train_ft_avg.npy      \n",
            "  inflating: X_train_ft_usif.npy     \n",
            "  inflating: X_train_glove_avg.npy   \n",
            "  inflating: X_train_glove_usif.npy  \n",
            "  inflating: X_val_ft_avg.npy        \n",
            "  inflating: X_val_ft_usif.npy       \n",
            "  inflating: X_val_glove_avg.npy     \n",
            "  inflating: X_val_glove_usif.npy    \n",
            "  inflating: lda_test.npy            \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LTTjE0vnQ3Sc"
      },
      "source": [
        "# Imports\n",
        "Here the imports are listed, which are important for the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "4-01uSWN0NBt",
        "outputId": "7b634a06-7930-49ef-a5aa-9d53c1686402"
      },
      "source": [
        "!pip install gensim\n",
        "!pip install -U git+https://github.com/oborchers/Fast_Sentence_Embeddings"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gensim in /usr/local/lib/python3.7/dist-packages (3.6.0)\n",
            "Requirement already satisfied: smart-open>=1.2.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (5.1.0)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.15.0)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from gensim) (1.19.5)\n",
            "Collecting git+https://github.com/oborchers/Fast_Sentence_Embeddings\n",
            "  Cloning https://github.com/oborchers/Fast_Sentence_Embeddings to /tmp/pip-req-build-1ay5f8vw\n",
            "  Running command git clone -q https://github.com/oborchers/Fast_Sentence_Embeddings /tmp/pip-req-build-1ay5f8vw\n",
            "Requirement already satisfied: numpy>=1.11.3 in /usr/local/lib/python3.7/dist-packages (from fse==0.1.16) (1.19.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.7/dist-packages (from fse==0.1.16) (1.4.1)\n",
            "Requirement already satisfied: smart_open>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from fse==0.1.16) (5.1.0)\n",
            "Requirement already satisfied: scikit-learn>=0.19.1 in /usr/local/lib/python3.7/dist-packages (from fse==0.1.16) (0.22.2.post1)\n",
            "Collecting gensim<4.0,>=3.8.0\n",
            "  Downloading gensim-3.8.3-cp37-cp37m-manylinux1_x86_64.whl (24.2 MB)\n",
            "\u001b[K     |████████████████████████████████| 24.2 MB 93 kB/s \n",
            "\u001b[?25hCollecting wordfreq>=2.2.1\n",
            "  Downloading wordfreq-2.5.0.tar.gz (56.8 MB)\n",
            "\u001b[K     |████████████████████████████████| 56.8 MB 25 kB/s \n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from fse==0.1.16) (5.4.8)\n",
            "Collecting Cython==0.29.14\n",
            "  Using cached Cython-0.29.14-cp37-cp37m-manylinux1_x86_64.whl (2.1 MB)\n",
            "Requirement already satisfied: six>=1.5.0 in /usr/local/lib/python3.7/dist-packages (from gensim<4.0,>=3.8.0->fse==0.1.16) (1.15.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.19.1->fse==0.1.16) (1.0.1)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.7/dist-packages (from wordfreq>=2.2.1->fse==0.1.16) (1.0.2)\n",
            "Collecting langcodes>=3.0\n",
            "  Downloading langcodes-3.1.0.tar.gz (168 kB)\n",
            "\u001b[K     |████████████████████████████████| 168 kB 72.0 MB/s \n",
            "\u001b[?25hCollecting regex>=2020.04.04\n",
            "  Downloading regex-2021.7.6-cp37-cp37m-manylinux2014_x86_64.whl (721 kB)\n",
            "\u001b[K     |████████████████████████████████| 721 kB 38.7 MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: fse, wordfreq, langcodes\n",
            "  Building wheel for fse (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fse: filename=fse-0.1.16-cp37-cp37m-linux_x86_64.whl size=656713 sha256=81c900be5d5adc9ca7d4c4c6e0da9b7391c4c42f58f9798597b7b99a060e1672\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-l9vcaw8s/wheels/2a/d4/e6/644ae9126bf0fd4d3c5059e5b49d2728e080b2dc389cad953b\n",
            "  Building wheel for wordfreq (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wordfreq: filename=wordfreq-2.5.0-py3-none-any.whl size=56830860 sha256=6eacde4aed5c33daddc6c918ce421a19f314735bde5f3871f9a87f570641d08d\n",
            "  Stored in directory: /root/.cache/pip/wheels/2f/5c/d1/84de857004256637e47a007da5fa6f4c069597aaf9f9ba4e6f\n",
            "  Building wheel for langcodes (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for langcodes: filename=langcodes-3.1.0-py3-none-any.whl size=165886 sha256=838f0d19fdf86ad31b7e4938344c61528d5dd4a5f9153824431d0003281ff9cd\n",
            "  Stored in directory: /root/.cache/pip/wheels/52/e1/07/5182862c67b7b982a9a6974e2c3a45cf1d4241cf3945e25fa7\n",
            "Successfully built fse wordfreq langcodes\n",
            "Installing collected packages: regex, langcodes, wordfreq, gensim, Cython, fse\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2019.12.20\n",
            "    Uninstalling regex-2019.12.20:\n",
            "      Successfully uninstalled regex-2019.12.20\n",
            "  Attempting uninstall: gensim\n",
            "    Found existing installation: gensim 3.6.0\n",
            "    Uninstalling gensim-3.6.0:\n",
            "      Successfully uninstalled gensim-3.6.0\n",
            "  Attempting uninstall: Cython\n",
            "    Found existing installation: Cython 0.29.23\n",
            "    Uninstalling Cython-0.29.23:\n",
            "      Successfully uninstalled Cython-0.29.23\n",
            "Successfully installed Cython-0.29.14 fse-0.1.16 gensim-3.8.3 langcodes-3.1.0 regex-2021.7.6 wordfreq-2.5.0\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "gensim"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "L2hIqHHCvjnP"
      },
      "source": [
        "import numpy\n",
        "import sklearn\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import KFold\n",
        "import tensorflow as tf\n",
        "#from tensorflow.keras.layers.experimental import preprocessing\n",
        "from tensorflow import keras\n",
        "import keras.backend as K\n",
        "from tensorflow.keras import layers\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from tensorflow.keras.layers.experimental.preprocessing import TextVectorization\n",
        "from sklearn.decomposition import NMF, LatentDirichletAllocation\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
        "import tensorflow_hub as hub\n",
        "\n",
        "#logger\n",
        "import logging\n",
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "#type annotation\n",
        "from typing import *\n",
        "tf.get_logger().setLevel('ERROR')\n",
        "\n",
        "import gc\n",
        "gc.enable()\n",
        "\n",
        "SEED = 100    #reproduceability\n",
        "from tensorflow.keras.layers import Embedding"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iSkxgNztW6oa"
      },
      "source": [
        "## Vectorisation (if needed)\n",
        "If already vectorised documents are used, this step can be ignored"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "diIlmixUX0CT"
      },
      "source": [
        "### Bow vectoriser and Sentence encoder"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_po6xl-85bV"
      },
      "source": [
        "def getVectoriser(kind:str = \"tfIdf\", vocab:number=512, ngram_range=(1,1), trainingsData=None):\n",
        "  \"\"\"This function creates the vectoriser according to the parameter and returns it.\n",
        "      It can be either a count, tfIdf or sentence vectoriser.\n",
        "\n",
        "    Args:\n",
        "        kind (str): The kind of the vectoriser such as 'tfIdf', 'sentence' or 'count'\n",
        "        vocab (number): The length of the vocabulary\n",
        "        ngram_range (any): The ngram range\n",
        "        trainingsData (any): The trainingsdata\n",
        "\n",
        "    Returns:\n",
        "        The vectoriser object\n",
        "    \"\"\"\n",
        "  global vectoriser_func\n",
        "  if kind is \"tfIdf\":\n",
        "    vectoriser = TfidfVectorizer(lowercase=True, min_df=2, max_features=vocab, ngram_range=ngram_range, stop_words=\"english\")\n",
        "    trainVectoriser(vectoriser, trainingsData)\n",
        "    return vectoriser\n",
        "  elif kind is \"count\":\n",
        "    vectoriser = CountVectorizer(lowercase=True, min_df=2, max_features=vocab, ngram_range=ngram_range, stop_words=\"english\")\n",
        "    trainVectoriser(vectoriser, trainingsData)\n",
        "    return vectoriser\n",
        "  elif kind is \"sentence\":\n",
        "    return hub.KerasLayer(\"https://tfhub.dev/google/universal-sentence-encoder/4\")\n",
        "\n",
        "\n",
        "def trainVectoriser(vectoriser, trainingsData):\n",
        "  \"\"\"This function is used to train the vectoriser\n",
        "\n",
        "    Args:\n",
        "        trainingsData (any): The data used for training\n",
        "        vectoriser (any): The vectoriser itself\n",
        "\n",
        "    Returns:\n",
        "        The vectorised data.\n",
        "    \"\"\"\n",
        "  \n",
        "  text_ds = trainingsData.flatten()\n",
        "  tmp = vectoriser.fit_transform(text_ds)\n",
        "  logger.info(tmp.shape)\n",
        "  del text_ds, tmp\n",
        "  gc.collect()\n",
        "\n",
        "def batchVectorisation(dfData, vectoriser, kindOfvectoriser:str):\n",
        "  \"\"\"This function loads the data in batches to reduce RAM requirenments.\n",
        "    Also it dstinguishes between 'sentence' for the sentence encoder and plain sklearn\n",
        "    vectorisers\n",
        "\n",
        "    Args:\n",
        "        dfData (any): The data of the dataframe\n",
        "        vectoriser (any): The vectoriser itself\n",
        "        kindOfvectoriser (str): This is the 'name' of the vectoriser, such as tfidf\n",
        "\n",
        "    Returns:\n",
        "        The vectorised data.\n",
        "    \"\"\"\n",
        "  \n",
        "  if kindOfvectoriser is \"sentence\":\n",
        "    vecFunc = lambda string: vectoriser(tf.constant(string)) \n",
        "  else:\n",
        "    vecFunc = lambda string: vectoriser.transform(string).toarray()\n",
        "  l = dfData[:,0]\n",
        "  r = dfData[:,1]\n",
        "  returnValue = [[],[]]\n",
        "  batch_size = 250\n",
        "  while len(l) >0 :\n",
        "    le = vecFunc(l[:batch_size]) #vectoriser.transform(l[:batch_size]).toarray()\n",
        "    re = vecFunc(r[:batch_size]) #vectoriser.transform(r[:batch_size]).toarray()\n",
        "\n",
        "    if len(returnValue[0]) == 0:\n",
        "      returnValue[0] = le\n",
        "      returnValue[1] = re\n",
        "    else:\n",
        "      returnValue[0] = numpy.concatenate((returnValue[0], le), axis=0)\n",
        "      returnValue[1] = numpy.concatenate((returnValue[1], re), axis=0)\n",
        "\n",
        "    l = l[batch_size:]\n",
        "    r = r[batch_size:]\n",
        "    logger.info(f\"{returnValue[0].shape} todo {l.shape}\")\n",
        "  return returnValue\n",
        "\n",
        "def vectoriseData(vectoriser, training, validation, kind:str = \"tfidf\"):\n",
        "  \"\"\"This function vectorises the data provided given the vectoriser provided.\n",
        "\n",
        "    Args:\n",
        "        vectoriser (any): This is the vectoriser object, it can be either be the\n",
        "        universal setnence encoder or one of the bow vectorisers\n",
        "        training (any): Is the trainingsdata data\n",
        "        validation (any): Is the validation data\n",
        "        kind (str): Is the data proviced\n",
        "\n",
        "    Returns:\n",
        "        The list of tokenised documents\n",
        "    \"\"\"\n",
        "  X_train_list = batchVectorisation(training, vectoriser, kind)\n",
        "  X_val_list = batchVectorisation(validation, vectoriser, kind)\n",
        "  return X_train_list, X_val_list"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iCvW72CsYhFw"
      },
      "source": [
        "### GloVe; Fasttext usif and avg document embeddings"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y3II5IBo7_4r"
      },
      "source": [
        "from gensim.models import Word2Vec, FastText\n",
        "!pip install -U fse\n",
        "from fse.models.average import FAST_VERSION\n",
        "print(FAST_VERSION) #1 means available\n",
        "from fse.models import Average, uSIF\n",
        "import gensim.downloader as api\n",
        "from fse import IndexedList\n",
        "\n",
        "class EmbeddingModels():\n",
        "  \"\"\"This class is used to create a Embedding model, which is able to vectorise documents.\n",
        "    Returns:\n",
        "        The document embeddings producing object\n",
        "    \"\"\"\n",
        "  def __init__(self, vocab_len, trainingsdata, embedName, combiName):\n",
        "    self.tokenizer = TfidfVectorizer(lowercase=True, min_df=2, max_features=vocab_len, ngram_range=(1,1), stop_words=\"english\").build_tokenizer()\n",
        "    tokdata = self.tokenize(trainingsdata)\n",
        "\n",
        "    self.embed = self.get_embedding(tokdata, embedName, vocab_len)\n",
        "    logger.error(\"got embedding\")\n",
        "\n",
        "    tokdata = IndexedList(tokdata)\n",
        "    logger.error(\"csp\")\n",
        "\n",
        "    self.model = self.getMergingMethod(combiName, self.embed, embedName, vocab_len)\n",
        "    logger.error(\"got combi\")\n",
        "    self.model.train(tokdata)\n",
        "    logger.error(\"finished training\")\n",
        "  \n",
        "  def getMergingMethod(self, combiName:str, embedding, embedName:str, voc_len:number):\n",
        "    \"\"\"This function returns the merging method for the word embeddings.\n",
        "\n",
        "    Args:\n",
        "        embedding (any): Serves the word embeddings\n",
        "        embedName (str): is the name of the embeddings\n",
        "        voc_len (number): is the lenght of the vocabulary\n",
        "        combiName (str): embedName is the name of the embeddings\n",
        "\n",
        "    Returns:\n",
        "        The Merging method\n",
        "    \"\"\"\n",
        "    if combiName is \"avg\":\n",
        "      return Average(embedding)\n",
        "    else:\n",
        "      if embedName is \"glove\":\n",
        "        return uSIF(embedding, workers=2, lang_freq=\"en\")\n",
        "      else:\n",
        "        return uSIF(embedding, length=voc_len)\n",
        "\n",
        "  def get_embedding(self, trainingsdata, embedName:str, vocab_len:number):\n",
        "    \"\"\"This function creates and returns an embedding for the given trainingsdata.\n",
        "\n",
        "    Args:\n",
        "        trainingsdata (List[str]): Is the data proviced\n",
        "        embedding (any): Serves the word embeddings\n",
        "        embedName (str): is the name of the embeddings\n",
        "        vocab_len (number): is the lenght of the vocabulary\n",
        "        combiName (str): embedName is the name of the embeddings\n",
        "\n",
        "    Returns:\n",
        "        The word level embedding function \n",
        "    \"\"\"\n",
        "    if embedName is \"glove\":\n",
        "      return api.load(\"glove-wiki-gigaword-100\")\n",
        "    elif embedName is \"fasttext\":\n",
        "      return FastText(window=3, min_count=1, sentences=trainingsdata, size=vocab_len)\n",
        "    else:\n",
        "      return Word2Vec(trainingsdata, min_count=1, size=vocab_len)\n",
        "\n",
        "  def tokenize(self, data)->list:\n",
        "    \"\"\"This function tokenises all the documents in the data.\n",
        "\n",
        "    Args:\n",
        "        data (any): Is the data proviced\n",
        "\n",
        "    Returns:\n",
        "        The list of tokenised documents\n",
        "    \"\"\"\n",
        "    return [self.tokenizer(doc) for doc in data]\n",
        "\n",
        "  def predict(self, data)->list:\n",
        "    \"\"\"This function creates the document embeddings\n",
        "\n",
        "    Args:\n",
        "        data (any): Is the data proviced\n",
        "\n",
        "    Returns:\n",
        "        The document embeddings\n",
        "    \"\"\"\n",
        "    tokenized_data = self.tokenize(data)\n",
        "    idxTok = IndexedList(tokenized_data)\n",
        "    inf_tokens = list(map(lambda tokenList: (tokenList, 0), tokenized_data))\n",
        "    #print(numpy.array(inf_tokens))\n",
        "    return list(map(lambda inf_data: self.model.infer([inf_data])[0], inf_tokens))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CmbR-ITkAajd"
      },
      "source": [
        "## LDA"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCet23LLr_pp"
      },
      "source": [
        "class NumberNormalizingVectorizer(TfidfVectorizer):\n",
        "  \"\"\"This class creates a custom vectoriser used for the LDA.\n",
        "\n",
        "    Returns:\n",
        "        A object of the Vectoriser\n",
        "    \"\"\"\n",
        "  def build_tokenizer(self):\n",
        "    tokenize = super().build_tokenizer()\n",
        "    return lambda doc: list(self.number_normalizer(tokenize(doc)))\n",
        "\n",
        "  def number_normalizer(self, tokens):\n",
        "    return (\"NUMBER\" if token[0].isdigit() else token for token in tokens)\n",
        "\n",
        "class GenerateLdaTopics():\n",
        "  \"\"\"\n",
        "  This Class is used to generate the LDA\n",
        "  \"\"\"\n",
        "  def __init__(self, _n_topics:number=200):\n",
        "    self.n_topics = _n_topics\n",
        "    self.cv = NumberNormalizingVectorizer(lowercase=True, min_df=5, max_features=20000, max_df=0.5, stop_words=\"english\",token_pattern='[a-zA-Z]+')\n",
        "    self.lda = LatentDirichletAllocation(n_components=_n_topics, random_state=0,n_jobs=-1)\n",
        "    self.vocab = None\n",
        "    \n",
        "  def fit(self, data):\n",
        "    \"\"\"This function is used to fit the LDA model.\n",
        "\n",
        "    Args:\n",
        "        data (any): The data passed to the vectoriser\n",
        "    \"\"\"\n",
        "    vectorised_data = self.cv.fit_transform(X_train.flatten())\n",
        "    logger.info(\"data vectorised\")\n",
        "    self.vocab = self.cv.get_feature_names()\n",
        "    logger.info(\"fitting lda\")\n",
        "    self.lda.fit(vectorised_data)\n",
        "\n",
        "  def predict(self, data):\n",
        "    \"\"\"This function returns the topic propabilities for the documents.\n",
        "\n",
        "    Args:\n",
        "        data (any): The data to produce the document-topic matri from\n",
        "\n",
        "    Returns:\n",
        "        The document-topic matrix\n",
        "    \"\"\"\n",
        "    vec_data = self.cv.transform(data)     #document-term matrix\n",
        "    return self.lda.transform(vec_data)    #document-topic matrix\n",
        "\n",
        "  def print_topics(self,n_top_words:number=5):\n",
        "    \"\"\"In order to visualise the topics,\n",
        "    this document prints representatives of the topics to show which words are contained within each topic.\n",
        "\n",
        "    Args:\n",
        "        n_top_words (number): The number words displayed for the topics\n",
        "\n",
        "    Returns:\n",
        "        A trainable model\n",
        "    \"\"\"\n",
        "    for topic, comp in enumerate(self.lda.components_):  #iterate through the topic-term matrix\n",
        "      word_idx = numpy.argsort(comp)[:-n_top_words-1:-1]\n",
        "      words = [self.vocab[i] for i in word_idx]\n",
        "      print('Topic: %d' % topic)\n",
        "      print('  %s' % ', '.join(words)) \n",
        "\n",
        "  def print_predictedTopics(self, data, n_top_topics:number = 5):\n",
        "    \"\"\"This function appends the vector length onto the vector itself.\n",
        "\n",
        "    Args:\n",
        "        data (number): The length of the documents\n",
        "        n_top_topics (number): The number of topics\n",
        "\n",
        "    Returns:\n",
        "        A trainable model\n",
        "    \"\"\"\n",
        "    pred = self.predict(data)\n",
        "    topicsAndPropabilities = []\n",
        "    for p in pred:    #for document in document-topic\n",
        "      top_topics = p.argsort()[:-n_top_topics - 1:-1]\n",
        "      propabilities = p[top_topics]\n",
        "      topicsAndPropabilities.append([top_topics, propabilities])#print k best topics\n",
        "    return topicsAndPropabilities"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ROtdSDkp_rjy"
      },
      "source": [
        "#The classifier\n",
        "To change the classifier, the concatenation layer in the *createModel* function has to be edited.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zUNZCa7aeZgp"
      },
      "source": [
        "class LayerAppendSum(tf.keras.layers.Layer):\n",
        "  \"\"\"This function appends the vector length onto the vector itself.\n",
        "    \"\"\"\n",
        "  def __init__(self):\n",
        "      super(LayerAppendSum, self).__init__()\n",
        "\n",
        "  def call(self, inputs):\n",
        "    \"\"\"This function gets automatically called with the data.\n",
        "\n",
        "    Args:\n",
        "        inputs (any): A vector within a vector of all the batches\n",
        "\n",
        "    Returns:\n",
        "        A concatenated vector length onto the vector \n",
        "    \"\"\"\n",
        "    def mapfunc(vec):\n",
        "      reduced_sum = tf.reduce_sum(vec)\n",
        "      reshaped = tf.reshape(reduced_sum, [-1])\n",
        "      return tf.concat([vec, reshaped], 0)\n",
        "    return tf.map_fn(lambda batch: mapfunc(batch), inputs)  #tf.concat([inputs, reduced])\n",
        "\n",
        "\n",
        "def createModel(vec_len:int):\n",
        "  \"\"\"This function creates a classifier for feature lists of the vec_len provided\n",
        "\n",
        "    Args:\n",
        "        vec_len (int): The length of the documents\n",
        "\n",
        "    Returns:\n",
        "        A trainable model\n",
        "    \"\"\"\n",
        "  ain = keras.Input(shape=(vec_len,), dtype=\"float\")\n",
        "  bin = keras.Input(shape=(vec_len,), dtype=\"float\")\n",
        "\n",
        "  a = tf.keras.layers.Lambda(lambda x: K.l2_normalize(x,axis=0))(ain)\n",
        "  b = tf.keras.layers.Lambda(lambda x: K.l2_normalize(x,axis=0))(bin)\n",
        "\n",
        "  avg_layer = tf.keras.layers.Average(name=\"avg\")([a, b])\n",
        "  mult_layer = tf.keras.layers.Multiply(name=\"mult\")([a,b])\n",
        "  sub_layer = tf.keras.layers.Subtract(name=\"sub\")([a, b])\n",
        "\n",
        "  cosine_sim = tf.keras.layers.Dot(axes=1, normalize=True, name=\"cosine_similarity\")([a,b])\n",
        "  #topic_sim = tf.keras.layers.Dot(axes=1, normalize=True, name=\"topic_similarity\")([x,y])\n",
        "\n",
        "  #avg_layer = LayerAppendSum()(avg_layer)\n",
        "  #mult_layer = LayerAppendSum()(mult_layer)\n",
        "  #sub_layer = LayerAppendSum()(sub_layer)\n",
        "\n",
        "  hidden = tf.keras.layers.concatenate([sub_layer, avg_layer, mult_layer,cosine_sim])#a,b , topic_sim])\n",
        "\n",
        "  outputs = layers.Dense(5, activation='softmax')(hidden)\n",
        "\n",
        "  model = keras.Model([ain,bin], outputs)   #,x,y\n",
        "  model.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=['accuracy'])\n",
        "  #model.compile(optimizer=\"adam\", loss=\"binary_crossentropy\", metrics=['accuracy']),\n",
        "  return model\n",
        "\n",
        "def download_model(model):\n",
        "  \"\"\"\n",
        "  This function is used to download models from google collab specifficly.\n",
        "  !tar -czvf model.tar.gz model/ is used for tarring the model\n",
        "  !tar -xvzf model.tar.gz is used for ent-tarring the model\n",
        "  \"\"\"\n",
        "  model.save(f\"./tmp_model\")\n",
        "  !tar -czvf tmp_model.tar.gz tmp_model/\n",
        "  from google.colab import files\n",
        "  files.download('tmp_model.tar.gz')\n",
        "\n",
        "def eval_model(predLabel:List[int], true_label:List[int]):\n",
        "  \"\"\"This function is used to determine the precision, recall and f1 score of a given model.\n",
        "\n",
        "    Args:\n",
        "        predLabel (List[int]): The predicted label of the model\n",
        "        predLabel (List[int]): The gold standard label\n",
        "\n",
        "    Returns:\n",
        "        The precission, recall, f1, and f1_macro scores\n",
        "    \"\"\"\n",
        "  f1_scores = sklearn.metrics.f1_score(y_true=true_label, y_pred=predLabel, average=None)\n",
        "  precission = sklearn.metrics.precision_score(y_true=true_label, y_pred=predLabel, average=None)\n",
        "  recall = sklearn.metrics.recall_score(y_true=true_label, y_pred=predLabel, average=None)\n",
        "\n",
        "  precission = list(map(lambda x: round(x, 4), precission))\n",
        "  recall = list(map(lambda x: round(x, 4), recall))\n",
        "  f1 = list(map(lambda x: round(x, 4), f1_scores))\n",
        "  f1_macro =  round(sum(f1)/len(f1), 4)\n",
        "  return precission, recall, f1, f1_macro"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OkLfsjhm812c"
      },
      "source": [
        "## Running the model configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Uq6b-CVsB_e",
        "outputId": "0eb56208-392c-450b-8554-40758e225d85"
      },
      "source": [
        "#newLda = GenerateLdaTopics()\n",
        "#newLda.fit(X_train.flatten())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "INFO:root:data vectorised\n",
            "INFO:root:fitting lda\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "enktq0-k6oqd"
      },
      "source": [
        "#### Finding the best vectoriser configuration"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xBxwsBFx68UB"
      },
      "source": [
        "def loadTrainAndValFeatures(nameTrainDataset: str, nameValDataset: str) -> list:\n",
        "  \"\"\"This function loads the training and validation data.\n",
        "\n",
        "    Args:\n",
        "        nameTrainDataset (str): The name of the training dataset\n",
        "        nameValDataset (str): The name of the validation dataset\n",
        "\n",
        "    Returns:\n",
        "        Both, the training and validation dataset\n",
        "    \"\"\"\n",
        "  X_train:numpy.ndarray = loadCSVData(\"X_train\")\n",
        "  X_val:numpy.ndarray = loadCSVData(\"X_val\")\n",
        "\n",
        "  return X_train, X_val\n",
        "\n",
        "def loadCSVData(name:str) -> numpy.ndarray:\n",
        "  \"\"\"This function loads SCV give the name.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the dataset\n",
        "\n",
        "    Returns:\n",
        "        The numpy array\n",
        "    \"\"\"\n",
        "  return pd.read_csv(f\"{name}.csv\")[[\"IssueA\", \"IssueB\"]].values.astype(\"str\")\n",
        "\n",
        "def loadNpData(name:str)-> numpy.ndarray:\n",
        "  \"\"\"This function loads numpy data give the name.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the dataset\n",
        "\n",
        "    Returns:\n",
        "        The numpy array\n",
        "    \"\"\"\n",
        "  return numpy.load(name+\".npy\", allow_pickle=True)\n",
        "\n",
        "def loadTrainAndValResults(nameTrainDataset: str, nameValDataset: str):\n",
        "  \"\"\"This function loads the training and validation results.\n",
        "\n",
        "    Args:\n",
        "        nameTrainDataset (str): The name of the training dataset\n",
        "        nameValDataset (str): The name of the validation dataset\n",
        "\n",
        "    Returns:\n",
        "        Both, the training and validation results\n",
        "    \"\"\"\n",
        "  Y_train = loadNpData(\"y_train\")\n",
        "  Y_val = loadNpData(\"y_val\")\n",
        "  Y_val = numpy.array([numpy.array(x) for x in Y_val])\n",
        "  Y_train = numpy.array([numpy.array(x) for x in Y_train])\n",
        "  return Y_train, Y_val\n",
        "\n",
        "\n",
        "def getVectorisedDataFromDataset(lda:bool, name:str, lda_name:str = None):\n",
        "  \"\"\"This function loads a specific X_train and X_val dataset and appends lda if needed.\n",
        "\n",
        "    Args:\n",
        "        lda (bool): Whether or not the model uses lds.\n",
        "        name (str): The The name of the data to load.\n",
        "        lda_name (str): The name of the lda datato load.\n",
        "\n",
        "    Returns:\n",
        "        The loaded data\n",
        "    \"\"\"\n",
        "  tmp_list = list(loadNpData(name))\n",
        "\n",
        "  if lda:\n",
        "    lda_data = list(loadNpData(lda_name))\n",
        "    tmp_list.append(lda_data[0])\n",
        "    tmp_list.append(lda_data[1])\n",
        "\n",
        "  return tmp_list\n",
        "\n",
        "def createVectorisedData(name:str,ngram,voc:int, lda:bool, X_train, X_val):\n",
        "  \"\"\"This function creates vectorised data, by creating a vectoriser given the requirenments specified and transforms the data given.\n",
        "\n",
        "    Args:\n",
        "        lda (bool): Whether or not the model uses lds.\n",
        "        name (str): The name of the data to load.\n",
        "        voc (int): The length of the vocabulary.\n",
        "        ngram (tuple): The ngram itself.\n",
        "        X_train (any): The training data.\n",
        "        X_val (any): The validation data.\n",
        "\n",
        "    Returns:\n",
        "        The training and validation data\n",
        "    \"\"\"\n",
        "  newLda= None\n",
        "  vectoriser = getVectoriser(trainingsData=X_train, vocab=voc, kind=name, ngram_range=ngram)\n",
        "  X_train_list, X_val_list = vectoriseData(vectoriser, X_train, X_val, kind=name)\n",
        "  if lda:\n",
        "    newLda = GenerateLdaTopics()\n",
        "    newLda.fit(X_train.flatten())\n",
        "    tl = newLda.predict(X_train[:,0])\n",
        "    tr = newLda.predict(X_train[:,1])\n",
        "    vl = newLda.predict(X_val[:,0])\n",
        "    vr = newLda.predict(X_val[:,1])\n",
        "\n",
        "    X_train_list.append(tl)\n",
        "    X_train_list.append(tr)\n",
        "    X_val_list.append(vl)\n",
        "    X_val_list.append(vr)\n",
        "\n",
        "  return X_train_list, X_val_list\n",
        "  \n",
        "def createAndEvaluateModel(X_train_list:list, X_val_list:list, Y_train, Y_val, voc:int):\n",
        "   \"\"\"This function creates and evaluates a model.\n",
        "\n",
        "    Args:\n",
        "        X_train_list (list): The training data.\n",
        "        X_val_list (list): The validation data.\n",
        "        Y_train (numpy.ndarray): The training solution.\n",
        "        Y_val (numpy.ndarray): The validation solution.\n",
        "        voc (int): The vocabulary size.\n",
        "\n",
        "    Returns:\n",
        "        The f1 scores evaluated\n",
        "    \"\"\"\n",
        "  model = createModel(voc)\n",
        "  model.fit(x:list=X_train_list, y=Y_train, batch_size:int = 25, epochs:int=25, validation_data=(X_val_list, Y_val))\n",
        "\n",
        "  output = model.predict(X_val_list, batch_size=15)\n",
        "  output = list(map(numpy.argmax, output))\n",
        "  y_val_int = list(map(numpy.argmax, Y_val))\n",
        "\n",
        "  return eval_model(output, y_val_int)\n",
        "\n",
        "\n",
        "def trainAndEvaluateModel(vectorisedData:bool, config, lda:bool):\n",
        "  \"\"\"\n",
        "  This function creates and evaluates multiple models.\n",
        "\n",
        "    Args:\n",
        "        vectorisedData (bool): The training data.\n",
        "        config (str): The validation data.\n",
        "        lda (bool): Whether or not LDA was used.\n",
        "\n",
        "    Returns:\n",
        "        The best vectoriser vonfiguration for the model\n",
        "    \"\"\"\n",
        "  precissions = []\n",
        "  recalls = []\n",
        "  f1s = []\n",
        "  f1_macros = []\n",
        "  Y_train, Y_val = loadTrainAndValResults(\"nameTrainDataset\", \"nameValDataset\")\n",
        "  \n",
        "  if vectorisedData:\n",
        "    for name in comfig:\n",
        "      X_train_list = getVectorisedDataFromDataset(lda, f\"X_train_{name}\")\n",
        "      X_val_list = getVectorisedDataFromDataset(lda, f\"X_val_{name}\")\n",
        "      voc = len(X_train_list[0][0])\n",
        "\n",
        "      precission, recall, f1, f1_macro = createAndEvaluateModel(X_train_list, X_val_list, Y_train, Y_val, voc)\n",
        "      precissions.append(precission)\n",
        "      recalls.append(recall)\n",
        "      f1s.append(f1)\n",
        "      f1_macros.append(f1_macro)\n",
        "      gc.collect()\n",
        "\n",
        "  else:\n",
        "    for name,ngram,voc in config:\n",
        "      X_train, X_val = loadTrainAndValFeatures(\"nameTrainDataset\", \"nameValDataset\")\n",
        "      X_train_list, X_val_list = createVectorisedData(name,ngram,voc, lda, X_train, X_val)\n",
        "\n",
        "      precission, recall, f1, f1_macro = createAndEvaluateModel(X_train_list, X_val_list, Y_train, Y_val, voc)\n",
        "      precissions.append(precission)\n",
        "      recalls.append(recall)\n",
        "      f1s.append(f1)\n",
        "      f1_macros.append(f1_macro)\n",
        "      gc.collect()\n",
        "  \n",
        "  npf1_macros = list(map(lambda x: round(x, 4), f1_macros))\n",
        "\n",
        "  bestIndex = npf1_macros.index(max(npf1_macros))\n",
        "  bestVecConfig = vectoriser_config[bestIndex]\n",
        "  #print(f\"\\nBest model config:\\n{bestVecConfig[0]}:{bestVecConfig[1]}:{bestVecConfig[2]} = {max(npf1_macros)}\")\n",
        "  print(f\"\\nBest model config:\\n{bestVecConfig} = {max(npf1_macros)}\")\n",
        "  return bestVecConfig"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gE96mFSP6ws0"
      },
      "source": [
        "#### Running the best vectoriser configuration\n",
        "The best vectoriser vonfiguration for the model is then run on the test dataset for evaluation purposes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lBNYeXUiEpFE"
      },
      "source": [
        "def printLatexOutput(output:numpy.ndarray, Y_test_int:numpy.ndarray, name:str, ngram, voc:int):\n",
        "  \"\"\"\n",
        "  This function creates a latex formated output to show the f1, precision, recall performances.\n",
        "\n",
        "    Args:\n",
        "        output (numpy.ndarray): The output of the classifier.\n",
        "        Y_test_int (numpy.ndarray): The integer values of the gold labels\n",
        "        name (str): The name of the vectorisation strategie.\n",
        "        ngram (tuple): The ngram.\n",
        "        voc (int): The vocabulary size.\n",
        "\n",
        "    Returns:\n",
        "        A latex formated output\n",
        "    \"\"\"\n",
        "  output = list(map(numpy.argmax, output))\n",
        "  precission, recall, f1, f1_macro = eval_model(output, Y_test_int)\n",
        "  labels = numpy.array(['unrelated', 'duplicate', '<=>', '<=', '=>'])\n",
        "  rearrange = numpy.array([4,3,2,0,1])\n",
        "  labels = labels[rearrange]\n",
        "  precission = numpy.array(precission)[rearrange]\n",
        "  recall = numpy.array(recall)[rearrange]\n",
        "  f1 = numpy.array(f1)[rearrange]\n",
        "\n",
        "  ng = \"\" if ngram[1] == 1 else \"_{,Un+Bigram}\"\n",
        "  output = f\"{name}_{{{voc}}}{ng}\" + \"\\t\\t\"\n",
        "  for j in range(len(labels)):\n",
        "    output += f\"& {precission[j]} & {recall[j]} \"\n",
        "  print(output + \"\\\\\\\\\")\n",
        "  output = f\"{name}_{{{voc}}}{ng}\" + \"\\t\\t\"\n",
        "\n",
        "  print(\"\\nf1 scores\\t\\t\"+ \"\\t\".join(labels) + \"\\tmacro f1\")\n",
        "  for j in range(len(labels)):\n",
        "    output += f\"& {f1[j]} \"\n",
        "  output += f\" & {f1_macro}\"\n",
        "  print(output + \"\\\\\\\\\")\n",
        "\n",
        "def vectoriseDataReturnVectoriser(name:str, ngram, voc:int, lda:bool):\n",
        "  \"\"\"\n",
        "  This function loads the training data and returns it in additino to the vectoriser and a LDA if present.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the vectorisation strategie.\n",
        "        ngram (tuple): The ngram.\n",
        "        voc (int): The vocabulary size.\n",
        "        lda (bool): Whether or not a LDA model was used.\n",
        "\n",
        "    Returns:\n",
        "        The trainings and validation data in addition to an vectoriser, and LDA\n",
        "    \"\"\"\n",
        "\n",
        "  X_train, X_val = loadTrainAndValFeatures(\"nameTrainDataset\", \"nameValDataset\")\n",
        "  Y_train, Y_val = loadTrainAndValResults(\"nameTrainDataset\", \"nameValDataset\")\n",
        "\n",
        "  vectoriser = getVectoriser(trainingsData=X_train, vocab=voc, kind=name, ngram_range=ngram)\n",
        "  X_train_list, X_val_list = vectoriseData(vectoriser, X_train, X_val, kind=name)\n",
        "  newLda = None\n",
        "  if lda:\n",
        "    newLda = GenerateLdaTopics()\n",
        "    newLda.fit(X_train.flatten())\n",
        "    tl = newLda.predict(X_train[:,0])\n",
        "    tr = newLda.predict(X_train[:,1])\n",
        "    vl = newLda.predict(X_val[:,0])\n",
        "    vr = newLda.predict(X_val[:,1])\n",
        "\n",
        "    X_train_list.append(tl)\n",
        "    X_train_list.append(tr)\n",
        "    X_val_list.append(vl)\n",
        "    X_val_list.append(vr)\n",
        "  return X_train_list, X_val_list, vectoriser, newLda\n",
        "\n",
        "def loadAndVectoriseTestData(name:str, vectoriser, lda:bool, newLda):\n",
        "  \"\"\"\n",
        "  This function cloads the test data and vectorises it using a given vectoriser.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the vectorisation strategie.\n",
        "        vectoriser (any): The vectoriser.\n",
        "        newLda (any): The LDA model or None.\n",
        "        lda (bool): Whether or not a LDA model was used.\n",
        "\n",
        "    Returns:\n",
        "        The vectorised data\n",
        "    \"\"\"\n",
        "  x_test:numpy.ndarray = loadCSVData(\"X_test\") \n",
        "  y_test:numpy.ndarray = loadNpData(\"y_test\")\n",
        "\n",
        "  X_test: List[numpy.ndarray] = batchVectorisation(x_test, vectoriser, name)#[test_dev[:,0],test_dev[:,1]]### list of numpy.ndarray[str]#\n",
        "  if lda:\n",
        "    tsl = newLda.predict(x_test[:,0])\n",
        "    tsr = newLda.predict(x_test[:,1])\n",
        "    X_test.append(tsl)\n",
        "    X_test.append(tsr)\n",
        "\n",
        "  Y_test_int:numpy.ndarray = numpy.array([numpy.argmax(numpy.array(tmp)) for tmp in y_test])\n",
        "  return X_test, Y_test_int\n",
        "\n",
        "def createBestModel(name:str,ngram,voc:int, lda:bool):\n",
        "  \"\"\"\n",
        "  This function creates the 'best' model, using the data provided.\n",
        "\n",
        "    Args:\n",
        "        name (str): The name of the vectorisation strategie.\n",
        "        ngram (tuple): The ngram.\n",
        "        voc (int): The vocabulary size.\n",
        "        lda (bool): Whether or not a LDA model was used.\n",
        "\n",
        "    Returns:\n",
        "        The 'best' model itself\n",
        "    \"\"\"\n",
        "  X_train_list, X_val_list, vectoriser, newLda = vectoriseDataReturnVectoriser(name, ngram, voc, lda)\n",
        "\n",
        "  model = createModel(voc)\n",
        "  model.fit(x=X_train_list, y=Y_train, batch_size = 25, epochs=25, validation_data=(X_val_list, Y_val))\n",
        "\n",
        "  del X_train, X_val, Y_train, Y_val\n",
        "  gc.collect()\n",
        "\n",
        "  X_test, Y_test_int = loadAndVectoriseTestData(name, vectoriser, lda, newLda)\n",
        "\n",
        "  output = model.predict(X_test, batch_size=15)\n",
        "  #print(numpy.array([f\"{rnd(x)} {numpy.argmax(x)}\" for x in output]))\n",
        "  printLatexOutput(output, Y_test_int, name, ngram, voc)\n",
        "  return model\n",
        " \n",
        "\n",
        "def rnd(num:float)->float:\n",
        "  \"\"\"\n",
        "  This function rounds the number given.\n",
        "\n",
        "    Args:\n",
        "        num (float): The number itself.\n",
        "\n",
        "    Returns:\n",
        "        The rounded number\n",
        "    \"\"\"\n",
        "  return list(map(lambda x: round(x, 4), x))\n",
        "\n",
        "vectoriser_config = [(\"count\", (1,1), 512), (\"tfIdf\", (1,1), 512), (\"count\", (1,2), 512), (\"tfIdf\", (1,2), 512), \n",
        "                     (\"count\", (1,1), 1024), (\"tfIdf\", (1,1), 1024), (\"count\", (1,2), 1024), (\"tfIdf\", (1,2), 1024),\n",
        "                     (\"count\", (1,1), 8192), (\"tfIdf\", (1,1), 8192), (\"count\", (1,2), 8192), (\"tfIdf\", (1,2), 8192),\n",
        "                     (\"count\", (1,1), 16384), (\"tfIdf\", (1,1), 16384), (\"count\", (1,2), 16384), (\"tfIdf\", (1,2), 16384),\n",
        "                     (\"sentence\",(1,1), 512)\n",
        "                     ]\n",
        "\n",
        "data_config = [\"ft_avg\", \"glove_avg\", \"ft_usif\", \"glove_usif\"]\n",
        "\n",
        "name,ngram,voc = trainAndEvaluateModel(False, vectoriser_config, False)\n",
        "model = createBestModel(name,ngram,voc, False)\n",
        "#trainAndEvaluateModel(True, data_config, False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dOvImfVokuca"
      },
      "source": [
        "tf.keras.utils.plot_model(\n",
        "    model,\n",
        "    to_file=\"Embed_baseline.png\",\n",
        "    show_shapes=True,\n",
        "    show_dtype=True,\n",
        "    show_layer_names=True,\n",
        "    rankdir=\"TB\",\n",
        "    expand_nested=False,\n",
        "    dpi=96,\n",
        ")"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}