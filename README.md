# Issue Relations
## Introduction
When developing microservice in large, a bug/issue occurring in one component might affect another component (a so called cross-component issue).
This might not be a problem for small software architectures, but when developing software in large, this poses a serious threat due to the developers not knowing every bug of every component.

## Problem Statement
Because of this threat, a automatic issue-relation classifier is needed, to automatically predict, which issues are dependent on other issues.

## Our Solution
First we identified five kinds of issue relations:
* _issue A leads to issue B_ (henceforth =>)
* _issue B leads to issue A_ (henceforth <=)
* _issue A and B are duplicates_ (henceforth dup.)
* _issue A and B are related but the direction is unknown_ (henceforth <=>)
* _issue A and B are unrelated_ 

And for those relations we provide a microservice based solution, which automatically creates a issue relation graph for the given issues.

## How We Addressed the Issue
### Annotators
First we had to extract issue relations by manually going through [repositories](./scraped_files/allRepos.txt) and finding mentions between issues.
Secondly a participant read over the issues and added the relation of the issues if (there was one) into a text file using the URL of both issues and their relation "symbol" (=>, <=>, ...).
Unrelated issues were not collected in this step, because they were generated later on.
After one participant finished the relations [data participant A](./issue_relations/data_participant_A.txt), a second participant filled out the blanks [data Participant B](./issue_relations/data_participant_B.txt), which were handed to him instead of the relation between the URLs.\
The inter rater agreement measured using the kohens kappa between both annotators was 0.70487 for the first 1457 issues, which is a substential agreement according to Landis and Koch.

### Scraper
The issue data (head, body, url) as well as the comments (body, date) were scraped from the repositories using the [scraping tool](./scraping_tool/).
In addition to that, the date of the relation was also collected to enable a matching of which comments were earlier and which later.\
[issue scraper](./scraping_tool/issueScraper/app.ts) is used to collect the issues.\
[comment scraper](./scraping_tool/commentScraper/commentScraper.ts) is used to collect the comments.\
[comment matcher](./scraping_tool/commentDateMatcher/reviewCommentDates.ts) appends the mention dates onto the relations.\

All those tools require node.js and can be run using the command *node [fiename]*.

### Creating Issue Relations
Both, the issues and their relation were then transformed into tuples *(IssueA, issueB, relation)* (the relation is a vector of the class and the issues themselves consist out of a concatenation of the issue title, body and the comments leading up to the date of the mention).\
Also, the unrelated issues were generated by creating the issue graph (unconnected a graph consisting out of smaller connected "sub" graphs) and pairing issues from different "sub" graphs together.\
Image of a connected sub-graph:
![image text](./relation_map/issue_map_cropped.png)
This ensures, that there is no transitive relation between the issues.
> A transitive relation can be seen in the picture above (the three green nodes at the top connected by "dupl." edges).
> lets call them A,B,C\
> now A is a duplicate of B, and B is a duplicate of C\
> Then A and C are probably also duplicates of each other, even though there is no edge between them.

### Relation Classifier
The classifier itself consists out of two parts, a vectoriser, which encodes the document in form of a vector and the classifier itself, for which we created a siamese like neuronal network.\

#### Vectoriser
For the vectoriser we tested several bag of words configurations like different vocabulary sizes, un- and bigrams as well as tf-idf weighting.\
In addition to that we tested fast text, and GloVe as word-level embeddings, combined using averaging them and usif as a weighted sentence/document representation.
Further, we used/tested the document embeddings obtained using the universal sentence encoder.

#### Classifier
For the classifier we created a siamese network, which takes in two inputs and creates an outputs the issue relation class.\
In order to evaluate the performance and what model works, we tested and evaluated several models.\
Using the Keras tuner to retrieve deep neuronal networks out of the models, did not lead to a improvement.

## Microservice
The microservice consists out of a [vectorsation service](./microservice/vectoriser/) and a [classification service](./microservice/classifier/).\
Both communicate over the following REST requests in a custom bridge network:\
There are 3 entrypoints for requests, the vectorise_issues and the vectorise_and_classify_issues entrypoint, the  for the Vectorisation service and the classify_documents one for the classification service.
For both services only the issue texts are required.\
The classification service on the other hand requires the issueTexts as well as the vectorisedIssues.

>http://localhost:5000/vectorise_issues
{"issueTexts": ["huehu", "aber ich bin ein doc", "anderes Document"]}

>http://localhost:5000/vectorise_and_classify_issues
{"issueTexts": ["huehu", "aber ich bin ein doc", "anderes Document"]}

>http://localhost:5001/classify_documents


The vectoriser uses the public available universal sentence encoder to create document embeddings, because it achieved the best results in combined with our _UniCosConcat_ classifier.\
And the classification service uses the _UniCosConcat_ model, and applies it onto all the relations needed to obtain the issue graph
s
## Instructions
### Instructions for the Microservice
The microservice can be started using docker-compose.
It requires docker, as well as compose.\
With the folder, the microservice can be started using the command *docker-compose up*.
However, the [universal sentence encoder v4](https://tfhub.dev/google/universal-sentence-encoder/4) haas to be downloaded in advance and placed into the *kubernetes_microservice/container1/app* folder.
Because in this folder the docker compose searches for a file named *universal-sentence-encoder_4.tar.gz*.\
After the images have been built using compose, they can be used in the kubernetes cluster.\
**IMPORTANT: We use the local images build using compose for our pods therefore the compose service has to be started in advance or the images have to built manually on advance**\
Simply run **kubectl append -f kompose.yaml** to push the images into the cluster.\
The microservice uses the following libraries: *numpy==1.19.2*, *tensorflow==2.5.0*, *httpx==0.18.2* and is build on top of the *tiangolo/uvicorn-gunicorn-fastapi:python3.8-2021-06-09* docker images, which creates a Uvicorn ASGI server for asynchronous python web code, in addition to Gunicorn, which manages Uvicorn and FastAPI in a single container.\
Important to note is, that the *MAX_WORKERS* environment variable can be set in the DOCERFILEs to enable multiple workers (or one per CPU).


### Instructions for the Jupiter Notebooks files
The Jupiter Notebooks files can be started by importing them into Google Collab.
Google Collab offers most the packages runs the code in the browser.\
*https://colab.research.google.com/*\

## Directory Structure

| Folder                                      | Explanation                                                                    |
| :------------------------------------------ | :----------------------------------------------------------------------------- |
| ISSUE_RELATIONS                             | This is the main folder                                                        |
| ├── [calculate_kappa](./calculate_kappa)    | Here lies the Kappa script to document the Kohens Kappa between the annotators |
| ├── [classifier](./classifier)              | This folder contains the code to generate the classifiers                      |
| ├── [generate_data](./generate_data)        | Here lies the logic to generate issue relations from the scraped data          |
| ├── [graph_plotting](./graph_plotting)      | This folder contains the script for plotting issue relation graphs             |
| ├── [issue_relations](./issue_relations)    | This folder contains the data gathered by the participants                     |
| ├┬─ [microservice](./microservice)          | Here lies the microservice                                                     |
| │└─ [classifier](./microservice/classifier) | Here lies the classification service of the microservice                       |
| │└─ [vectoriser](./microservice/vectoriser) | Here lies the vectoriser service of the microservice                           |
| ├── [statistics](./statistics)              | This folder contains images of the Corpus statistics                           |
| ├── [relation_map](./relation_map)          | This folder contains images of the issue relation map                          |
| ├── [scraped_files](./scraped_files)        | Here the issues & relation gathered are lying                                  |
| ├┬─ [scraping_tool](./scraping_tool)        | This folder contains the tools to scrape the GitHub issues                     |
| │└─ [commentDateMatcher](./scraping_tool/commentDateMatcher) | Here lies the tool for finding and adding the comment dates of the mentions                                          |
| │└─ [commentDateMatcher](./scraping_tool/commentScraper) | This scraper is used to get all the comments of the Issues                                          |
| │└─ [commentDateMatcher](./scraping_tool/issueScraper) | Here lies the issueScraper, which scrapes the issues from GitHub                                          |
| │└─ [modules_and_interfaces](./scraping_tool/modules_and_interfaces) | Contains the modules and interfaces used for the scraping and storing of the data                                          |
| └── [trainingsdata](./trainingsdata)        | This folder contains the training, validation and test data as well as statistics regarding those |
